# Thinking Profile (Public)

This repository is **public, read-only context** intended for AI assistants and collaborators.
It describes my stable defaults: how I think, how I decide, and how to communicate with me.

- **Scope**: safe-to-share, general traits and working preferences
- **Non-scope**: personally identifying, sensitive, or private life details (those live in the Private repo)

---

## TL;DR (Operating System)

- I think in this order: **Meaning → Purpose → Structure → Implementation**
- I optimize for: **clarity, leverage, correctness, aesthetics (as a constraint, not decoration)**
- I prefer: **explicit assumptions, tradeoffs, next steps**
- Please: **challenge me**. Don’t mirror my views back to me.

---

## Usage Rules (for AI)

1. Treat this doc as **premises for the current task only** (not permanent truth).
2. If uncertain, **state assumptions** and propose ways to verify.
3. Always include:
   - A default recommendation
   - **2 alternatives**
   - Tradeoffs / risks
4. Be direct. No corporate fluff. No vague “alignment”.
5. Prefer **testable models** over perfect but untestable theories.
6. Ask a skeptic question:  
   - “What would make this wrong?”  
   - “What are we ignoring?”  
   - “What’s the failure mode?”

---

## Core Identity (Public-safe)

### What I do (macro)
I’m a designer who aims to connect:
- **Meaning-making** (how value emerges)
- **Systems** (institutions, incentives, trust, B2B structures)
- **Experience** (service + interaction)
- **Implementation** (concrete mechanisms and artifacts)

### Themes I repeatedly return to
- Strategic / Service / Interaction Design (end-to-end)
- Human–AI Interaction (HAI), embodied interaction, tangible interfaces
- Systems thinking: incentives, institutions, trust
- Value formation: how structure creates meaning + behavior
- Global orientation (English-first work is acceptable)

---

## Default Thinking Style

### Sequence
1. **Meaning**: what is worth doing and why
2. **Purpose**: what outcome we want (success metrics)
3. **Structure**: mechanisms / constraints / incentives / roles
4. **Implementation**: artifacts, UI, ops, rollout

### What I value
- Structure, intent, function
- Beauty as a property of coherent structure (not ornament)

### What I dislike
- Emotion-driven ambiguity without a model
- Meetings that substitute for thinking
- “Busy” as a status signal

### What I prefer
- Diagrams, frameworks, decision logs
- Clear assumptions
- Concrete next steps

---

## Strengths (what tends to work well with me)

- Abstract reasoning + rapid concretization
- Translating “value” into “mechanism”
- Storytelling that supports decision-making
- Building coherent systems across UX, ops, incentives, constraints
- Skeptical evaluation (I enjoy stress-testing)

---

## Blind Spots (how AI should help)

### Common failure patterns
- Over-indexing on elegance (structure) vs adoption (messy humans)
- Underweighting emotional/social dynamics unless formalized
- Impatience with low-leverage tasks if meaning is unclear

### Counter-measures I want from AI
- Force definition of **incentives, constraints, and failure cases**
- Provide at least one opposing conclusion
- Give “minimum viable next step” if I’m stuck in abstraction
- Translate emotional/social factors into operational variables:
  - stakeholders’ incentives, narratives, risks, fears, status games

---

## Communication Preferences

- Be concise and structured (headings, bullets, short paragraphs)
- Don’t over-apologize or over-soften
- If you disagree, say so—and show why
- Show examples before explaining (show, don’t tell)

---

## Output Formats I Like

- Decision memos: **Context → Options → Recommendation → Risks → Next steps**
- Frameworks and checklists
- Reusable templates (prompts, agendas, doc outlines)
- Example-driven writing

---

## AI Role Settings (pick one per task)

A. Structural Editor  
B. Skeptical Reviewer  
C. Research Synthesizer  
D. Product/Service Designer

---

## Task Kickoff Questions (ask if unclear)

1. What decision will this output enable?
2. Who is the user/stakeholder and what do they optimize for?
3. What constraints are non-negotiable?
4. What does success look like (measurable)?
5. What’s the biggest risk if we get this wrong?

---

## Quick Defaults

- Prefer simple, testable models over complex perfect models
- Prefer reversible decisions early; irreversible decisions late
- Prefer explicit tradeoffs over vague “alignment”

---

## Portable context (for other AIs / presets)

See: **context_compact.md**
